{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Loss: 0.0506\n",
      "Epoch [1/100] Loss: 0.0380\n",
      "Epoch [2/100] Loss: 0.0361\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CelebAデータセットのカスタムデータセットクラス\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.dataset = ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx][0]\n",
    "\n",
    "# VAEモデルの定義\n",
    "class VAE_CelebA(nn.Module):\n",
    "    def __init__(self, in_channels: int, latent_dim: int, hidden_dims: List = None):\n",
    "        super(VAE_CelebA, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # エンコーダーの構築\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "        # エンコーダーの出力サイズを計算\n",
    "        self.feature_size = self._get_feature_size(64)\n",
    "\n",
    "        # エンコーダーの出力サイズに基づいて全結合層を定義\n",
    "        self.fc_mu = nn.Linear(self.feature_size, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.feature_size, latent_dim)\n",
    "\n",
    "        # デコーダーの構築\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Linear(latent_dim, self.feature_size)\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride=2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                               hidden_dims[-1],\n",
    "                               kernel_size=3,\n",
    "                               stride=2,\n",
    "                               padding=1,\n",
    "                               output_padding=1),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], out_channels=3,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def _get_feature_size(self, img_size):\n",
    "        x = torch.zeros(1, 3, img_size, img_size)\n",
    "        x = self.encoder(x)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    def encode(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2)  # ここが問題であれば修正\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        result = (result + 1.) / 2.\n",
    "        return result\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), input, mu, log_var\n",
    "\n",
    "    def loss_function(self,\n",
    "                      recons: torch.Tensor, \n",
    "                      input: torch.Tensor, \n",
    "                      mu: torch.Tensor, \n",
    "                      log_var: torch.Tensor, \n",
    "                      kld_weight: float = 0.00025) -> torch.Tensor:\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim=1), dim=0)\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return loss\n",
    "\n",
    "# データ変換の定義\n",
    "IMAGE_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE, antialias=True),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# データセットとデータローダの作成\n",
    "data_dir = '/home/data/hnakai/CelebA'\n",
    "dataset = CelebADataset(root_dir=data_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# モデル、オプティマイザ、ロス関数の設定\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = VAE_CelebA(in_channels=3, latent_dim=512).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# トレーニングループ\n",
    "num_epochs = 100\n",
    "model.train()\n",
    "epoch_losses = []  # エポックごとのロスを保存するリスト\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0  # エポックごとのロスを記録する変数\n",
    "    for batch_idx, images in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recons, input, mu, log_var = model(images)\n",
    "        loss = model.loss_function(recons, input, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()  # バッチごとのロスを累積\n",
    "    epoch_losses.append(epoch_loss / len(dataloader))  # エポックごとの平均ロスを計算して保存\n",
    "    print(f'Epoch [{epoch}/{num_epochs}] Loss: {epoch_losses[-1]:.4f}')\n",
    "\n",
    "# トレーニング終了後、モデルを保存\n",
    "torch.save(model.state_dict(), 'vae_celeba.pth')\n",
    "\n",
    "# ロスのプロット\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), epoch_losses, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# モデルのテストと入力画像の再構成\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, images in enumerate(dataloader):\n",
    "        if batch_idx == 0:  # 最初のバッチだけ使用\n",
    "            images = images.to(device)\n",
    "            recons, _, _, _ = model(images)\n",
    "            break\n",
    "\n",
    "    # 再構成された画像と元の画像を表示\n",
    "    for i in range(5):\n",
    "        original = transforms.ToPILImage()(images[i].cpu())\n",
    "        reconstructed = transforms.ToPILImage()(recons[i].cpu())\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2)\n",
    "        axes[0].imshow(original)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[1].imshow(reconstructed)\n",
    "        axes[1].set_title(\"Reconstructed\")\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Counterfactual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
